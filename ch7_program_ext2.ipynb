{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27966b2b",
   "metadata": {},
   "source": [
    "指令微调扩展实验2：更改指令微调提示词的风格，原来用的是Alpaca风格，现在改成Phi-3的风格试下效果\n",
    "\n",
    "```\n",
    "# \"<|user|>\" 和 \"<|assistant|>\" 并不在GPT2的词表中，所以在经过tokenizer处理后，得到的不是一个tokenid，而是拆成多个\n",
    "# 所以就有两种思路：\n",
    "# 思路1：扩大词表，增加special token：\"<|user|>\" 和 \"<|assistant|>\"，同时模型的嵌入层和最终的out_head层也需要扩充\n",
    "# 思路2：就当作普通的词处理，直接构建提示词模板，然后微调\n",
    "\n",
    "# 接下来试下思路2\n",
    "```   \n",
    "\n",
    "结论：\n",
    "1. 经过指令微调后，模型还是能够很好的遵循输出格式的，比如<|assistant|> 之后才是正式的回复。  \n",
    "2. 训练的曲线上看，loss下降不如Alpaca风格，好像更容易过拟合。\n",
    "3. 同样利用llama3:8b做评分，得分低于Alpaca风格的指令模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5c13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12ce3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n",
      "[27, 91, 7220, 91, 29]\n",
      "[27, 91, 7220, 91, 29]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
    "print(encoded)\n",
    "\n",
    "encoded = tokenizer.encode(\"<|user|>\", allowed_special={\"<|user|>\"})\n",
    "print(encoded)\n",
    "\n",
    "encoded = tokenizer.encode(\"<|user|>\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"<|user|>\" 和 \"<|assistant|>\" 并不在GPT2的词表中，所以在经过tokenizer处理后，得到的不是一个tokenid，而是拆成多个\n",
    "# 所以就有两种思路：\n",
    "# 思路1：扩大词表，增加special token：\"<|user|>\" 和 \"<|assistant|>\"，同时模型的嵌入层和最终的out_head层也需要扩充\n",
    "# 思路2：就当作普通的词处理，直接构建提示词模板，然后微调\n",
    "\n",
    "# 接下来试下思路2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc0aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo entries: 1100\n"
     ]
    }
   ],
   "source": [
    "## 加载数据\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w',encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, 'r',encoding='utf-8') as f:\n",
    "            text_data = f.read()\n",
    "    # 注释：返回文本数据\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://github.com/MLNLP-World/LLMs-from-scratch-CN/tree/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number fo entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c925054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the following sentence to use a metaphor', 'input': \"'Her eyes are like stars.'\", 'output': 'Her eyes are stars in the night sky.'}\n",
      "{'instruction': 'Convert this sentence to passive voice', 'input': 'The chef cooked a delicious meal.', 'output': 'A delicious meal was cooked by the chef.'}\n",
      "{'instruction': \"Convert the following verb to its past participle form: 'go'\", 'input': 'go', 'output': \"The past participle form of 'go' is 'gone'.\"}\n",
      "{'instruction': 'Given the following sentence, rewrite it by removing the passive form', 'input': 'The final decision was made by the manager.', 'output': 'The manager made the final decision.'}\n",
      "{'instruction': \"Transform the following sentence into a question using 'should.'\", 'input': 'You need to see a doctor.', 'output': 'Should you see a doctor?'}\n",
      "{'instruction': 'Given the following sentence, rewrite it by removing the passive form', 'input': 'The concert tickets are being sold quickly.', 'output': 'They are selling the concert tickets quickly.'}\n",
      "{'instruction': \"Rewrite the following sentence to include a hyperbole: 'I'm very tired.'\", 'input': \"I'm very tired.\", 'output': \"I'm so tired I could sleep for a year.\"}\n",
      "{'instruction': 'Classify this sentence into one of the following topics: health, economy, technology, music', 'input': 'Spotify has revolutionized how we listen to music.', 'output': 'The topic of the given sentence is music.'}\n",
      "{'instruction': 'Reverse this array', 'input': '[10, 20, 30, 40, 50]', 'output': 'Here is the reversed array: [50, 40, 30, 20, 10]'}\n",
      "{'instruction': \"Convert the following sentence to passive voice: 'The committee approved the new guidelines.'\", 'input': 'The committee approved the new guidelines.', 'output': 'The new guidelines were approved by the committee.'}\n",
      "{'instruction': \"Rewrite the sentence using a metaphor: 'The assignment was a breeze.'\", 'input': 'The assignment was a breeze.', 'output': 'The assignment was a piece of cake.'}\n",
      "{'instruction': 'Classify the following statement into one of these labels: [positive, negative, neutral]', 'input': 'My computer is working perfectly.', 'output': \"The statement 'My computer is working perfectly' can be classified as positive.\"}\n",
      "{'instruction': \"Convert the sentence to passive voice: 'The chef prepared the meal.'\", 'input': 'The chef prepared the meal.', 'output': 'The meal was prepared by the chef.'}\n",
      "{'instruction': 'Identify the verb in the following sentence', 'input': 'The cat sleeps on the couch.', 'output': 'The verb in the sentence is \"sleeps.\"'}\n",
      "{'instruction': 'Reverse this array', 'input': '[5, 15, 25, 35, 45]', 'output': 'Here is the reversed array: [45, 35, 25, 15, 5]'}\n",
      "{'instruction': \"Convert the sentence to passive voice: 'The teacher explains the lesson.'\", 'input': 'The teacher explains the lesson.', 'output': 'The lesson is explained by the teacher.'}\n",
      "{'instruction': 'Classify this sentence into one of the following topics: health, economy, technology, music', 'input': 'Electric cars are revolutionizing the automotive industry.', 'output': 'The topic of the given sentence is technology.'}\n",
      "{'instruction': \"Transform the following sentence into a question using 'would.'\", 'input': 'You can lend me a pen.', 'output': 'Would you lend me a pen?'}\n",
      "{'instruction': 'Convert the following verb to its infinitive form', 'input': 'went', 'output': \"The infinitive form of 'went' is 'to go'.\"}\n",
      "{'instruction': 'Classify the following statement into one of these labels: [positive, negative, neutral]', 'input': 'My computer crashed.', 'output': 'Negative'}\n",
      "{'instruction': 'Rewrite this sentence using appropriate grammar rules:', 'input': 'he saw a old man', 'output': 'He saw an old man.'}\n",
      "{'instruction': \"Rewrite the following sentence to include a hyperbole: 'I'm very hungry.'\", 'input': \"I'm very hungry.\", 'output': \"I'm so hungry I could eat a horse.\"}\n",
      "{'instruction': \"Rewrite the sentence to improve clarity: 'He saw that the sky was clouds full.'\", 'input': 'He saw that the sky was clouds full.', 'output': 'He saw that the sky was full of clouds.'}\n",
      "{'instruction': 'Convert this sentence to passive voice', 'input': 'The gardener watered the plants.', 'output': 'The plants were watered by the gardener.'}\n",
      "{'instruction': 'Transform the following sentence into a question using \"could.\"', 'input': 'You can help me tomorrow.', 'output': 'Could you help me tomorrow?'}\n",
      "{'instruction': \"Convert the following verb to its past participle form: 'speak'\", 'input': 'speak', 'output': \"The past participle form of 'speak' is 'spoken.'\"}\n",
      "{'instruction': 'Classify the following statement into one of these labels: [positive, negative, neutral]', 'input': 'My computer crashed.', 'output': 'The statement \"My computer crashed\" can be classified as negative.'}\n",
      "{'instruction': \"Convert the sentence to passive voice: 'They will hold the meeting tomorrow.'\", 'input': 'They will hold the meeting tomorrow.', 'output': 'The meeting will be held tomorrow by them.'}\n",
      "{'instruction': 'Find the missing number in the sequence', 'input': '2, 4, ?, 8, 10', 'output': 'The missing number in the sequence is 6.'}\n",
      "{'instruction': 'Identify the adverb in the sentence', 'input': 'She quickly ran to the store.', 'output': \"The adverb in the sentence is 'quickly'.\"}\n"
     ]
    }
   ],
   "source": [
    "for entry in data:\n",
    "    if entry['input'] != '':\n",
    "        if entry['instruction'][-1] != '.':\n",
    "            print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33299cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Change the sentence 'You should have called me.' into a question.\",\n",
       " 'input': '',\n",
       " 'output': 'Should you have called me?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44338e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Change the sentence 'You should have called me.' into a question:\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry['instruction'][:-1]+':'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3ba3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction = ''\n",
    "    input = ''\n",
    "    if entry['input'] == '':\n",
    "        instruction = entry['instruction']\n",
    "    else:\n",
    "        input = f\"{entry['input']}\"\n",
    "        if entry['instruction'][-1] == '.':\n",
    "            instruction = entry['instruction'][:-1]+':'\n",
    "        else:\n",
    "            instruction = entry['instruction'] + ':'\n",
    "\n",
    "\n",
    "    instruction_input_text = (\n",
    "        f\"<|user|>\\n{instruction} {input}\"\n",
    "    )\n",
    "    return instruction_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51ac3524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Edit the following sentence for grammar.',\n",
       " 'input': 'He go to the park every day.',\n",
       " 'output': 'He goes to the park every day.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47e5af5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Suggest a more formal synonym for \"happy.\" \n",
      "----------\n",
      "<|user|>\n",
      "Edit the following sentence for grammar: He go to the park every day.\n"
     ]
    }
   ],
   "source": [
    "print(format_input(data[5]))\n",
    "print(\"----------\")\n",
    "print(format_input(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d5f9aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4fc089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokkenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n<|assistant|>\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokkenizer.encode(full_text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4055a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
    "print(encoded)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c56866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Evaluate the following phrase by transforming it into the spelling given: freind --> friend\n",
      "\n",
      "<|assistant|>\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "print(tokenizer.decode(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d83c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "# 函数参数预填充\n",
    "from functools import partial\n",
    "\n",
    "custimized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_szie = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d679b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "MODEL_CONFIG= {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, context_length, drop_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        # 拆多头\n",
    "        queries = queries.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        values = values.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        # 调整位置\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # 计算注意力分数\n",
    "        att_scores = queries @ keys.transpose(-2, -1) / keys.shape[-1] ** 0.5\n",
    "        att_scores.masked_fill_(self.mask[:seq_len, :seq_len], -torch.inf)\n",
    "        att_weights = torch.softmax(att_scores, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "        context_vec = att_weights @ values # shape (B,H,L,D)\n",
    "        # 头调整回去\n",
    "        context_vec = context_vec.transpose(1,2).contiguous().view(batch_size, seq_len, dim)\n",
    "        out = self.W_o(context_vec)\n",
    "        return out\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, emb_dim * 4)\n",
    "        self.fc2 = nn.Linear(emb_dim * 4, emb_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.att = MultiHeadAttention(cfg['emb_dim'], cfg['n_heads'], cfg['context_length'], cfg['drop_rate'], cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        tok_embed = self.tok_emb(input_ids)\n",
    "        pos_embed = self.pos_emb(torch.arange(seq_len, device=input_ids.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        loggits = self.out_head(x)\n",
    "        return loggits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6173f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model:nn.Module, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # 取最后一个token\n",
    "        logits = logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "670f8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# Listing 5.5 Loading OpenAI weights into our GPT model code\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt:GPTModel, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])               #A\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    for b in range(len(params[\"blocks\"])):                                       #B\n",
    "        q_w, k_w, v_w = np.split(                                                #C\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_k.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_v.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_k.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_v.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.W_o.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.W_o.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc1.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc1.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.fc2.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc2.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])                   #D\n",
    "\n",
    "\n",
    "#A 将模型的位置嵌入和token 嵌入的权重设置为 params 中指定的值\n",
    "#B 遍历模型中的每个 Transformer 模块\n",
    "#C 使用 np.split 函数将注意力和偏置权重分为三等份，分别用于查询、键和值组件\n",
    "#D OpenAI 的原始 GPT-2 模型在输出层中复用了 token 嵌入的权重，以减少参数总量，这一概念称为权重共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "615deccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d09cceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.' \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "061a9d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The chef cooks the meal every day.'\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "Convert the active sentence to\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer), max_new_tokens=35, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97df8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch) # shape (B,L,V)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.permute(0,2,1), target_batch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "698fd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "876d558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.7118, Val loss: 3.6345\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c40eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.724, Val loss 2.702\n",
      "Ep 1 (Step 000005): Train loss 1.574, Val loss 1.416\n",
      "Ep 1 (Step 000010): Train loss 1.133, Val loss 1.289\n",
      "Ep 1 (Step 000015): Train loss 1.153, Val loss 1.249\n",
      "Ep 1 (Step 000020): Train loss 1.048, Val loss 1.215\n",
      "Ep 1 (Step 000025): Train loss 1.030, Val loss 1.178\n",
      "Ep 1 (Step 000030): Train loss 1.101, Val loss 1.136\n",
      "Ep 1 (Step 000035): Train loss 0.993, Val loss 1.110\n",
      "Ep 1 (Step 000040): Train loss 0.943, Val loss 1.087\n",
      "Ep 1 (Step 000045): Train loss 0.883, Val loss 1.082\n",
      "Ep 1 (Step 000050): Train loss 0.900, Val loss 1.060\n",
      "Ep 1 (Step 000055): Train loss 1.042, Val loss 1.037\n",
      "Ep 1 (Step 000060): Train loss 0.989, Val loss 1.019\n",
      "Ep 1 (Step 000065): Train loss 0.898, Val loss 1.008\n",
      "Ep 1 (Step 000070): Train loss 0.751, Val loss 1.009\n",
      "Ep 1 (Step 000075): Train loss 0.797, Val loss 1.013\n",
      "Ep 1 (Step 000080): Train loss 0.838, Val loss 0.997\n",
      "Ep 1 (Step 000085): Train loss 0.673, Val loss 0.971\n",
      "Ep 1 (Step 000090): Train loss 0.751, Val loss 0.955\n",
      "Ep 1 (Step 000095): Train loss 0.683, Val loss 0.944\n",
      "Ep 1 (Step 000100): Train loss 0.690, Val loss 0.937\n",
      "Ep 1 (Step 000105): Train loss 0.765, Val loss 0.931\n",
      "Ep 1 (Step 000110): Train loss 0.740, Val loss 0.928\n",
      "Ep 1 (Step 000115): Train loss 0.695, Val loss 0.916\n",
      "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.'   <|assistant|> The active sentence is 'The chef cooks the meal every day.'<|endoftext|>The following is a list of the most commonly used words in the English language.  1. The word 'dove' is\n",
      "Ep 2 (Step 000120): Train loss 0.591, Val loss 0.922\n",
      "Ep 2 (Step 000125): Train loss 0.600, Val loss 0.945\n",
      "Ep 2 (Step 000130): Train loss 0.601, Val loss 0.945\n",
      "Ep 2 (Step 000135): Train loss 0.560, Val loss 0.955\n",
      "Ep 2 (Step 000140): Train loss 0.558, Val loss 0.957\n",
      "Ep 2 (Step 000145): Train loss 0.511, Val loss 0.946\n",
      "Ep 2 (Step 000150): Train loss 0.516, Val loss 0.933\n",
      "Ep 2 (Step 000155): Train loss 0.581, Val loss 0.929\n",
      "Ep 2 (Step 000160): Train loss 0.563, Val loss 0.941\n",
      "Ep 2 (Step 000165): Train loss 0.508, Val loss 0.945\n",
      "Ep 2 (Step 000170): Train loss 0.451, Val loss 0.938\n",
      "Ep 2 (Step 000175): Train loss 0.465, Val loss 0.919\n",
      "Ep 2 (Step 000180): Train loss 0.521, Val loss 0.907\n",
      "Ep 2 (Step 000185): Train loss 0.565, Val loss 0.912\n",
      "Ep 2 (Step 000190): Train loss 0.467, Val loss 0.903\n",
      "Ep 2 (Step 000195): Train loss 0.451, Val loss 0.889\n",
      "Ep 2 (Step 000200): Train loss 0.416, Val loss 0.883\n",
      "Ep 2 (Step 000205): Train loss 0.477, Val loss 0.874\n",
      "Ep 2 (Step 000210): Train loss 0.479, Val loss 0.877\n",
      "Ep 2 (Step 000215): Train loss 0.538, Val loss 0.879\n",
      "Ep 2 (Step 000220): Train loss 0.409, Val loss 0.894\n",
      "Ep 2 (Step 000225): Train loss 0.456, Val loss 0.907\n",
      "Ep 2 (Step 000230): Train loss 0.405, Val loss 0.907\n",
      "<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.'   <|assistant|> The meal is cooked every day by the chef.<|endoftext|>The following is a list of the most commonly used words in the English language.  1. cat  2. cat  3.\n",
      "Total training time: 1.41 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=5, eval_iter=5, start_context=format_input(val_data[0]), tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_min = (end_time - start_time) / 60\n",
    "print(f\"Total training time: {execution_time_min:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6b6c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fe9d123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "680e34f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Convert the active sentence to passive: 'The chef cooks the meal every day.'\",\n",
       " 'input': '',\n",
       " 'output': 'The meal is cooked by the chef every day.'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac5b2c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Classify an input string as either a noun or a verb: Dance\n",
      "\n",
      "<|assistant|>\n",
      "Noun: Dance\n",
      "Vocational: To dance\n",
      "<|assistant|>\n",
      "Vocational: To dance\n"
     ]
    }
   ],
   "source": [
    "input_text = format_input(val_data[1])\n",
    "token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer).to(device), max_new_tokens=256, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ecf26f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: Dance\n",
      "Vocational: To dance\n",
      "\n",
      "Vocational: To dance\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].replace(\"<|assistant|>\",\"\").strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2f2bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:00<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer).to(device), max_new_tokens=256, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = generated_text[len(input_text):].replace(\"<|assistant|>\",\"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "#     json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "53463609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a cheetah.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "12ac00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"llama3:8b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\":[\n",
    "            {\"role\":\"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\":{\n",
    "            \"seed\":123,\n",
    "            \"temperature\":0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9fc3b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(test_data, json_key, model=\"llama3:8b\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(test_data):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`,\"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score:{score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a398a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:18<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores:110 of 110\n",
      "Average score: 50.66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores:{len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
