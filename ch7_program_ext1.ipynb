{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d47ad1",
   "metadata": {},
   "source": [
    "指令微调扩展实验1：微调训练的时候增加PAD Token Mask, 对PAD Token不做注意力计算\n",
    "\n",
    "> 实验结果：没任何影响。\n",
    "\n",
    "原因，问了下Gemini3-Pro：\n",
    "\n",
    "我在训练GPT2模型的时候，发现在多头注意力计算模块中，应用padding mask和不应用padding mask没区别，这是为何呢？\n",
    "\n",
    "---  \n",
    "\n",
    "这是一个非常深刻且在实际训练中容易被忽视的问题。如果你发现在训练 GPT-2（或类似的 Causal Decoder-only 模型）时，应用 Padding Mask（填充掩码）和不应用它似乎没有区别，通常是由以下几个核心原因造成的。\n",
    "\n",
    "为了深入理解这个问题，我们需要先回顾 GPT-2 的注意力机制和掩码的工作原理。\n",
    "\n",
    "### 核心背景：两种掩码的区别\n",
    "\n",
    "在 GPT-2 这种 Decoder-only 的架构中，主要涉及两种掩码：\n",
    "\n",
    "1.  **Causal Mask (Look-ahead Mask / Triangular Mask):**\n",
    "    *   **作用：** 防止模型“偷看”未来的 Token。位置 $i$ 只能看到位置 $0$ 到 $i$ 的信息。\n",
    "    *   **形状：** 一个下三角矩阵（对角线及以下为 1，上方为 0 或负无穷）。\n",
    "    *   **必要性：** **绝对必须**。没有它，模型就变成了双向的（像 BERT），无法进行自回归生成。\n",
    "\n",
    "2.  **Padding Mask (Key Padding Mask):**\n",
    "    *   **作用：** 告诉模型哪些 Token 是填充的（`<pad>`），不应该参与计算注意力分数。\n",
    "    *   **形状：** 通常是一个布尔矩阵，标记哪些位置是有效的，哪些是 Padding。\n",
    "    *   **必要性：** 理论上需要，但在 GPT 类模型中，**其重要性往往被高估或被 Causal Mask 隐式处理了**。\n",
    "\n",
    "---\n",
    "\n",
    "### 原因分析：为何 Padding Mask 看起来没区别？\n",
    "\n",
    "以下是导致这一现象的四个主要原因，按可能性从高到低排列：\n",
    "\n",
    "#### 1. Causal Mask 的“屏蔽”效应 (最常见原因)\n",
    "\n",
    "这是最根本的原因。在 GPT-2 的自注意力机制中，**Causal Mask 已经限制了 Token 只能关注它左边的内容**。\n",
    "\n",
    "*   **场景：** 假设你的 Batch 数据如下（`P` 代表 Padding）：\n",
    "    `[A, B, C, P, P]`\n",
    "*   **对于有效 Token (A, B, C)：**\n",
    "    *   A 只能看 A。\n",
    "    *   B 只能看 A, B。\n",
    "    *   C 只能看 A, B, C。\n",
    "    *   **关键点：** 有效的 Token (A, B, C) **永远在 Padding (P) 的左边**（如果我们使用右侧填充）。由于 Causal Mask 的存在，A、B、C 根本就没有机会去“看”右边的 P。\n",
    "*   **结论：** 只要你使用的是**右侧填充 (Right Padding)**，对于所有非 Padding 的有效 Token 来说，Padding Mask 是完全多余的，因为 Causal Mask 已经把右边的 Padding 挡住了。\n",
    "\n",
    "#### 2. Padding Token 的 Loss 被忽略了\n",
    "\n",
    "在计算最终的 Loss（通常是 CrossEntropyLoss）时，我们通常会设置 `ignore_index`。\n",
    "\n",
    "*   **计算过程：**\n",
    "    1.  模型计算所有位置的输出（包括 Padding 位置）。\n",
    "    2.  虽然 Padding 位置可能会因为没有 Padding Mask 而去关注前面的有效 Token，从而产生非零的输出向量。\n",
    "    3.  **但是**，在计算 Loss 时，我们通常设置 `loss_fct = CrossEntropyLoss(ignore_index=pad_token_id)`。\n",
    "    4.  这意味着，虽然 Padding 位置计算出了“错误的”或“无意义的”注意力，但这些位置产生的预测结果**根本不参与梯度的反向传播**。\n",
    "*   **结论：** 既然 Padding 位置的输出不影响 Loss，那么它们是否正确地 mask 掉前面的信息也就无关紧要了。\n",
    "\n",
    "#### 3. 注意力分数的 Softmax 特性\n",
    "\n",
    "即使在极少数情况下（例如左侧填充，或者 Padding 在中间），Padding Mask 没有加上，Softmax 也会起到一定的平滑作用。\n",
    "\n",
    "*   如果 Padding Token 的 Embedding 向量是随机初始化的，它们与 Query 的点积可能很小。\n",
    "*   经过 Softmax 后，分配给 Padding 的权重可能本身就很低（虽然不是严格的 0）。这会稀释影响，使得模型看起来依然在收敛，虽然性能可能会受到微小的、难以察觉的负面影响。\n",
    "\n",
    "#### 4. 数据预处理方式 (Packing / Concatenation)\n",
    "\n",
    "很多现代 LLM 训练（如 LLaMA 训练流程）为了提高效率，不再使用传统的 Padding。\n",
    "\n",
    "*   **做法：** 将多条短文本拼接（Concat）在一起，中间用 `<eos>` 分隔，直到填满 `max_seq_len`。\n",
    "*   **结果：** 这种情况下，Batch 中几乎没有 Padding Token（或者极少）。\n",
    "*   **结论：** 如果你的数据处理逻辑是这样的，那么 Padding Mask 自然就没有存在的意义，因为根本没有 Padding。\n",
    "\n",
    "---\n",
    "\n",
    "### 什么时候 Padding Mask 是必须的？\n",
    "\n",
    "虽然在标准的 GPT-2 右侧填充训练中它看起来没用，但在以下情况中它是**必须**的：\n",
    "\n",
    "1.  **左侧填充 (Left Padding)：**\n",
    "    *   通常用于推理（Generation）阶段。如果你的 Batch 是 `[P, P, A, B, C]`，那么 A 在计算注意力时，如果不加 Mask，就会看到左边的 P。这会干扰 A 的特征表示。\n",
    "2.  **双向模型 (BERT/Encoder)：**\n",
    "    *   BERT 允许看上下文，如果不 Mask 掉 Padding，模型会利用 Padding 的信息，导致严重错误。\n",
    "3.  **Cross-Attention (Encoder-Decoder 架构)：**\n",
    "    *   在 Transformer 的 Decoder 中，Cross-Attention 模块关注 Encoder 的输出。这时必须把 Encoder 输出中的 Padding Mask 掉，否则 Decoder 会关注到 Encoder 的废弃部分。\n",
    "\n",
    "### 总结与建议\n",
    "\n",
    "**结论：**\n",
    "如果在训练 GPT-2 时使用的是**右侧填充 (Right Padding)** 并且正确设置了 Loss 函数的 **`ignore_index`**，那么**不应用 Padding Mask 对训练结果确实没有影响**。这是因为 Causal Mask 已经阻止了有效 Token 看到右侧的 Padding，而 Loss 计算又忽略了 Padding 位置本身的输出。\n",
    "\n",
    "**建议：**\n",
    "尽管如此，为了代码的健壮性和兼容性（防止未来切换到左侧填充或修改架构），**最佳实践依然是加上 Padding Mask**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "decb1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "MODEL_CONFIG= {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, context_length, drop_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x, padding_mask=None): # padding_mask shape: (B,L)\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        # 拆多头\n",
    "        queries = queries.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        values = values.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        # 调整位置\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # 计算注意力分数\n",
    "        att_scores = queries @ keys.transpose(-2, -1) / keys.shape[-1] ** 0.5\n",
    "        att_scores.masked_fill_(self.mask[:seq_len, :seq_len], -torch.inf)\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.view(batch_size, 1, 1, -1)\n",
    "            att_scores.masked_fill_(padding_mask, -torch.inf)\n",
    "        att_weights = torch.softmax(att_scores, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "        context_vec = att_weights @ values # shape (B,H,L,D)\n",
    "        # 头调整回去\n",
    "        context_vec = context_vec.transpose(1,2).contiguous().view(batch_size, seq_len, dim)\n",
    "        out = self.W_o(context_vec)\n",
    "        return out\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, emb_dim * 4)\n",
    "        self.fc2 = nn.Linear(emb_dim * 4, emb_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.att = MultiHeadAttention(cfg['emb_dim'], cfg['n_heads'], cfg['context_length'], cfg['drop_rate'], cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x, padding_mask = inputs\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x, padding_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        return (x, padding_mask)\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, input_ids, pad_token_id=50256):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        padding_mask = input_ids == pad_token_id\n",
    "        tok_embed = self.tok_emb(input_ids)\n",
    "        pos_embed = self.pos_emb(torch.arange(seq_len, device=input_ids.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.drop_emb(x)\n",
    "        x,_ = self.trf_blocks((x, padding_mask))\n",
    "        x = self.final_norm(x)\n",
    "        loggits = self.out_head(x)\n",
    "        return loggits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03b120c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    2,    3,    4,    5,    6, -100, -100]])\n",
      "tensor([[False, False, False, False, False, False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "test_tokenids = torch.tensor([[1,2,3,4,5,6,-100,-100]])\n",
    "print(test_tokenids)\n",
    "test_tokenids_mask = test_tokenids == -100\n",
    "print(test_tokenids_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c833d1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68753ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10, 10), diagonal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2b87efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2315,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.8956, 0.9594,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.3896, 0.6360, 0.3435,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4110, 0.2196, 0.2884, 0.7004,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.0831, 0.6955, 0.4569, 0.5083, 0.5049,   -inf,   -inf,   -inf],\n",
       "        [0.3639, 0.3376, 0.5094, 0.5533, 0.0158, 0.9663,   -inf,   -inf],\n",
       "        [0.6063, 0.8651, 0.7173, 0.8124, 0.3056, 0.8466, 0.0579,   -inf],\n",
       "        [0.0011, 0.8377, 0.6703, 0.7992, 0.4780, 0.8317, 0.8205, 0.7092]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(8,8).masked_fill_(torch.triu(torch.ones(8, 8), diagonal=1).bool(), -torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7742fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0468,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.5085, 0.8450,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.5393, 0.9506, 0.5013,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1254, 0.0291, 0.1227, 0.0293,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4296, 0.5999, 0.7586, 0.8983, 0.9573,   -inf,   -inf,   -inf],\n",
      "        [0.5348, 0.1726, 0.3836, 0.9099, 0.7770, 0.1242,   -inf,   -inf],\n",
      "        [0.1058, 0.7375, 0.8985, 0.0397, 0.1927, 0.7766, 0.3893,   -inf],\n",
      "        [0.9766, 0.5614, 0.0111, 0.8473, 0.4994, 0.7738, 0.8179, 0.8941]])\n",
      "tensor([False, False, False, False, False, False,  True,  True])\n",
      "tensor([[0.0468,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.5085, 0.8450,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.5393, 0.9506, 0.5013,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1254, 0.0291, 0.1227, 0.0293,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4296, 0.5999, 0.7586, 0.8983, 0.9573,   -inf,   -inf,   -inf],\n",
      "        [0.5348, 0.1726, 0.3836, 0.9099, 0.7770, 0.1242,   -inf,   -inf],\n",
      "        [0.1058, 0.7375, 0.8985, 0.0397, 0.1927, 0.7766,   -inf,   -inf],\n",
      "        [0.9766, 0.5614, 0.0111, 0.8473, 0.4994, 0.7738,   -inf,   -inf]])\n"
     ]
    }
   ],
   "source": [
    "# 先不考虑批次维度，先看下如何将padding mask也加入到att scores中\n",
    "att_scores = torch.rand(8,8).masked_fill_(torch.triu(torch.ones(8, 8), diagonal=1).bool(), -torch.inf)\n",
    "print(att_scores)\n",
    "test_tokenids = torch.tensor([1,2,3,4,5,6,-100,-100])\n",
    "test_tokenids_mask = test_tokenids == -100\n",
    "print(test_tokenids_mask)\n",
    "att_scores.masked_fill_(test_tokenids_mask, -torch.inf)\n",
    "print(att_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e7d4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# Listing 5.5 Loading OpenAI weights into our GPT model code\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt:GPTModel, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])               #A\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    for b in range(len(params[\"blocks\"])):                                       #B\n",
    "        q_w, k_w, v_w = np.split(                                                #C\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_k.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_v.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_k.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_v.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.W_o.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.W_o.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc1.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc1.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.fc2.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc2.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])                   #D\n",
    "\n",
    "\n",
    "#A 将模型的位置嵌入和token 嵌入的权重设置为 params 中指定的值\n",
    "#B 遍历模型中的每个 Transformer 模块\n",
    "#C 使用 np.split 函数将注意力和偏置权重分为三等份，分别用于查询、键和值组件\n",
    "#D OpenAI 的原始 GPT-2 模型在输出层中复用了 token 嵌入的权重，以减少参数总量，这一概念称为权重共享\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d0017e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49de923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo entries: 1100\n"
     ]
    }
   ],
   "source": [
    "## 加载数据\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w',encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, 'r',encoding='utf-8') as f:\n",
    "            text_data = f.read()\n",
    "    # 注释：返回文本数据\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://github.com/MLNLP-World/LLMs-from-scratch-CN/tree/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number fo entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be3b7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca风格\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf805b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afaa83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokkenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokkenizer.encode(full_text))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be776cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dc89924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "# 函数参数预填充\n",
    "from functools import partial\n",
    "\n",
    "custimized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_szie = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0176566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7803da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model:nn.Module, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # 取最后一个token\n",
    "        logits = logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a9875af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26586264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer), max_new_tokens=35, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab44f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch) # shape (B,L,V)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.permute(0,2,1), target_batch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4f4732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c3547f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 3.8248, Val loss: 3.7609\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89be6e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.639, Val loss 2.628\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.873, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.858\n",
      "Ep 1 (Step 000030): Train loss 0.797, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.716, Val loss 0.809\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.807\n",
      "Ep 1 (Step 000045): Train loss 0.634, Val loss 0.791\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.784\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.720, Val loss 0.744\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.736\n",
      "Ep 1 (Step 000070): Train loss 0.535, Val loss 0.731\n",
      "Ep 1 (Step 000075): Train loss 0.568, Val loss 0.731\n",
      "Ep 1 (Step 000080): Train loss 0.607, Val loss 0.726\n",
      "Ep 1 (Step 000085): Train loss 0.508, Val loss 0.711\n",
      "Ep 1 (Step 000090): Train loss 0.560, Val loss 0.690\n",
      "Ep 1 (Step 000095): Train loss 0.499, Val loss 0.680\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.563, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.552, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.506, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The meal is prepared by the chef.<|endoftext|>The meal is prepared by the chef.<|endoftext|>The meal is prepared by the chef.<|endoftext|>The meal is prepared by the chef.\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.449, Val loss 0.688\n",
      "Ep 2 (Step 000130): Train loss 0.445, Val loss 0.684\n",
      "Ep 2 (Step 000135): Train loss 0.403, Val loss 0.684\n",
      "Ep 2 (Step 000140): Train loss 0.408, Val loss 0.682\n",
      "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.413, Val loss 0.678\n",
      "Ep 2 (Step 000160): Train loss 0.414, Val loss 0.686\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.688\n",
      "Ep 2 (Step 000170): Train loss 0.322, Val loss 0.684\n",
      "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.672\n",
      "Ep 2 (Step 000180): Train loss 0.388, Val loss 0.657\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.658\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
      "Ep 2 (Step 000195): Train loss 0.328, Val loss 0.637\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
      "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.636\n",
      "Ep 2 (Step 000210): Train loss 0.361, Val loss 0.636\n",
      "Ep 2 (Step 000215): Train loss 0.394, Val loss 0.640\n",
      "Ep 2 (Step 000220): Train loss 0.299, Val loss 0.649\n",
      "Ep 2 (Step 000225): Train loss 0.339, Val loss 0.660\n",
      "Ep 2 (Step 000230): Train loss 0.291, Val loss 0.656\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The chef prepares the meal every day.<|endoftext|>The meal was cooked by the chef every day.<|endoftext|>The meal was prepared by the chef every day.<|endoftext|>The meal\n",
      "Total training time: 32.56 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=5, eval_iter=5, start_context=format_input(val_data[0]), tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_min = (end_time - start_time) / 60\n",
    "print(f\"Total training time: {execution_time_min:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "498606ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df75674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f73b74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:09<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer).to(device), max_new_tokens=256, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "#     json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "841c5b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Rewrite the sentence using a simile.',\n",
       " 'input': 'The car is very fast.',\n",
       " 'output': 'The car is as fast as lightning.',\n",
       " 'model_response': 'The car is as fast as a bullet.'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4063aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"llama3:8b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\":[\n",
    "            {\"role\":\"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\":{\n",
    "            \"seed\":123,\n",
    "            \"temperature\":0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "063d641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(test_data, json_key, model=\"llama3:8b\"):\n",
    "    scores = []\n",
    "    for entry in test_data:\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`,\"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score:{score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11d98cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores:110 of 110\n",
      "Average score: 52.38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores:{len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
