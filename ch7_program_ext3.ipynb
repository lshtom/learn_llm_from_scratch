{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a74a62",
   "metadata": {},
   "source": [
    "指令微调扩展实验3：在计算loss的时候，将指令和输入部分掩码（-100），不计损失，观察下效果\n",
    "\n",
    "> 实验结果：一样的超参数配置，训练loss能收敛到更小，而验证loss似乎更早的平缓了，更容易过拟合? 不过最终效果上看，要优于指令和输入不做掩码的情形。 \n",
    "\n",
    "--- \n",
    "这个问题问Gemini3Pro，回答如下：   \n",
    "\n",
    "\n",
    "> \n",
    "在对 GPT-2（以及绝大多数 Decoder-only 架构的语言模型，如 LLaMA、Bloom 等）进行指令微调（Instruction Fine-tuning）时，关于是否对“指令+输入”部分进行掩码（Masking），这是一个非常关键的细节。\n",
    "\n",
    "简短的回答是：**通常建议对“指令+输入”部分进行掩码，只计算“输出（Response/Target）”部分的 Loss。**\n",
    "\n",
    "以下是详细的分析、对比以及原因解释。\n",
    "\n",
    "### 1. 两种方式的定义\n",
    "\n",
    "假设我们有一条训练数据：\n",
    "*   **Instruction (指令):** \"将下面的句子翻译成英文。\"\n",
    "*   **Input (输入):** \"今天天气很好。\"\n",
    "*   **Target (目标输出):** \"The weather is good today.\"\n",
    "\n",
    "拼接后的完整序列（Sequence）是：\n",
    "`[BOS] 将下面的句子翻译成英文。今天天气很好。[SEP] The weather is good today. [EOS]`\n",
    "\n",
    "#### 方式 A：不做掩码 (Full Language Modeling Loss)\n",
    "*   模型在预测序列中的每一个 token 时，都会计算 Loss。\n",
    "*   模型不仅要学习如何生成 \"The weather is good today.\"，还要学习如何生成指令 \"将下面的句子翻译成英文...\"。\n",
    "\n",
    "#### 方式 B：做掩码 (Masking Instruction/Input)\n",
    "*   我们在计算 Loss 时，将 Instruction 和 Input 对应的 token 的 Loss 权重设为 0（通常在代码中通过 `labels` 设为 -100 来实现）。\n",
    "*   模型只针对 \"The weather is good today.\" 这一部分的预测结果计算梯度并更新参数。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 哪种方式更好？为什么？\n",
    "\n",
    "**结论：方式 B（做掩码，只计算输出部分的 Loss）通常更好。**\n",
    "\n",
    "主要原因如下：\n",
    "\n",
    "#### (1) 训练目标的对齐 (Alignment of Training Objective)\n",
    "指令微调的核心目的是让模型学会**“根据指令执行任务”**。\n",
    "*   **如果不做掩码**：模型实际上是在做两个任务。一是“复述/生成指令”，二是“执行指令”。这会分散模型的注意力。\n",
    "*   **如果做掩码**：模型的唯一目标就是 $P(\\text{Response} | \\text{Instruction}, \\text{Input})$。这强迫模型关注于如何根据上文生成正确的下文，而不是去记忆上文本身的概率分布。\n",
    "\n",
    "#### (2) 避免“指令过拟合”与分布偏差\n",
    "在指令微调数据集中，指令往往存在大量的重复模板（例如，“请把这段话摘要一下”、“翻译成中文”等）。\n",
    "*   **如果不做掩码**：模型会花费大量的梯度去学习这些高频出现的指令模板。这会导致 Loss 虚低（因为指令很容易预测），掩盖了真正难学的 Response 部分的 Loss。模型可能会变得非常擅长生成“请把...”这几个字，但在实际回答问题上提升有限。\n",
    "*   **如果做掩码**：模型忽略指令本身的生成概率，专注于难以预测的答案部分。\n",
    "\n",
    "#### (3) 符合推理时的行为 (Inference Consistency)\n",
    "在实际使用（推理）阶段，用户会给出 Instruction 和 Input，模型只需要生成 Response。模型永远不需要在推理时去预测用户会输入什么指令。因此，训练时计算指令部分的 Loss 是没有实际应用价值的，甚至可能引入噪声。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 技术实现细节 (PyTorch/Hugging Face 示例)\n",
    "\n",
    "在 Hugging Face `transformers` 库中，计算 Loss 时通常会自动忽略 `labels` 中值为 `-100` 的位置。\n",
    "\n",
    "假设你已经将文本 Tokenize 成了 `input_ids`，你需要构建对应的 `labels`。\n",
    "\n",
    "#### 逻辑步骤：\n",
    "1.  对 `Instruction + Input` 进行 tokenize，得到长度为 `L1` 的序列。\n",
    "2.  对 `Target` (Response) 进行 tokenize，得到长度为 `L2` 的序列。\n",
    "3.  拼接 `input_ids` = `[Instruction_ids] + [Target_ids]`。\n",
    "4.  构建 `labels`：\n",
    "    *   前 `L1` 个位置设为 `-100` (Mask)。\n",
    "    *   后 `L2` 个位置设为 `Target_ids`。\n",
    "\n",
    "#### 代码示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# 确保有 pad_token，GPT2 默认没有\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def build_inputs_with_mask(instruction, input_text, target):\n",
    "    # 1. 构建 Prompt\n",
    "    prompt = f\"{instruction}\\n{input_text}\\nAnswer:\"\n",
    "    \n",
    "    # 2. 编码 Prompt 和 Target\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    target_ids = tokenizer.encode(target, add_special_tokens=False) + [tokenizer.eos_token_id]\n",
    "    \n",
    "    # 3. 拼接 Input IDs\n",
    "    input_ids = prompt_ids + target_ids\n",
    "    \n",
    "    # 4. 构建 Labels (关键步骤)\n",
    "    # Prompt 部分设为 -100，表示计算 Loss 时忽略\n",
    "    # Target 部分保留原 ID\n",
    "    labels = [-100] * len(prompt_ids) + target_ids\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "# 示例数据\n",
    "data = {\n",
    "    \"instruction\": \"Translate to French.\",\n",
    "    \"input\": \"Hello world.\",\n",
    "    \"target\": \"Bonjour le monde.\"\n",
    "}\n",
    "\n",
    "processed = build_inputs_with_mask(data[\"instruction\"], data[\"input\"], data[\"target\"])\n",
    "\n",
    "print(\"Input IDs:\", processed[\"input_ids\"])\n",
    "print(\"Labels:   \", processed[\"labels\"])\n",
    "\n",
    "# 模拟输入模型\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# output = model(input_ids=processed[\"input_ids\"].unsqueeze(0), labels=processed[\"labels\"].unsqueeze(0))\n",
    "# loss = output.loss # 这里计算的 loss 自动忽略了 -100 的部分\n",
    "```\n",
    "\n",
    "### 4. 特殊情况：什么时候“不做掩码”也可以？\n",
    "\n",
    "虽然做掩码是主流做法（如 Alpaca, Vicuna 等训练方式），但在以下少数情况，不做掩码也是一种选择：\n",
    "\n",
    "1.  **预训练数据极其匮乏**：如果你是在做一个特定领域的 Continue Pre-training（持续预训练），且你的指令数据本身也是高质量的领域文本，你希望模型同时学习指令中的领域知识。\n",
    "2.  **数据格式不是标准的 Q&A**：如果你的数据更像是一段连续的对话流，且你希望模型具备“提出问题”的能力（即模拟用户），那么计算全序列 Loss 是有意义的。\n",
    "\n",
    "### 总结\n",
    "\n",
    "对于标准的**指令微调（Instruction Tuning）**任务，**强烈建议对指令和输入部分进行掩码（Masking）**。这能让模型更专注于学习如何生成正确的回复，加速收敛，并提升最终的指令遵循能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d284e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667dd4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number fo entries: 1100\n"
     ]
    }
   ],
   "source": [
    "## 加载数据\n",
    "\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w',encoding='utf-8') as f:\n",
    "            f.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, 'r',encoding='utf-8') as f:\n",
    "            text_data = f.read()\n",
    "    # 注释：返回文本数据\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://github.com/MLNLP-World/LLMs-from-scratch-CN/tree/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number fo entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fdd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca风格\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87854f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a631f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokkenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        self.mask_lens = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(tokkenizer.encode(full_text))\n",
    "            self.mask_lens.append(len(tokkenizer.encode(instruction_plus_input)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.encoded_texts[idx],self.mask_lens[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e515e59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f4a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InstructionDataset(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76b13196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n### Response:\\nThe spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[0][0][train_dataset[0][1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "[198, 21017, 18261,    25,   198, 29284, 22979,   262, 12187,    13]\n",
    "\n",
    "[198,198, 21017, 18261,    25,   198, 29284, 22979,   262, 12187,    13]\n",
    "\n",
    " [198,   198, 21017, 18261,    25,   198,   464, 22918,   286, 26600, 318,  6032,  1088,   362,    13,    20,    13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "150c953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item[0]) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for data in batch:\n",
    "        item, mask_len = data\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        # 除了PAD掩码外，指令+输入也掩码，计算loss时只考虑Response部分\n",
    "        #mask[:mask_len-1] = True # 因为标签是原始内容左移1位得到的，所以实际指令+输入部分也左移了一位 (补充：写在这里有问题)\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "            targets[:mask_len-1] = ignore_index # 将指令+输入部分掩码\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "# 函数参数预填充\n",
    "from functools import partial\n",
    "\n",
    "custimized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_szie = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_szie, collate_fn=custimized_collate_fn, shuffle=False, num_workers=num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c958df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "MODEL_CONFIG= {\n",
    "    'vocab_size': 50257,\n",
    "    'context_length': 1024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_layers': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads, context_length, drop_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1).bool())\n",
    "        self.W_q = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(emb_dim, emb_dim, bias=qkv_bias)\n",
    "        self.W_o = nn.Linear(emb_dim, emb_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        # 拆多头\n",
    "        queries = queries.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        values = values.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        # 调整位置\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        # 计算注意力分数\n",
    "        att_scores = queries @ keys.transpose(-2, -1) / keys.shape[-1] ** 0.5\n",
    "        att_scores.masked_fill_(self.mask[:seq_len, :seq_len], -torch.inf)\n",
    "        att_weights = torch.softmax(att_scores, dim=-1)\n",
    "        att_weights = self.dropout(att_weights)\n",
    "        context_vec = att_weights @ values # shape (B,H,L,D)\n",
    "        # 头调整回去\n",
    "        context_vec = context_vec.transpose(1,2).contiguous().view(batch_size, seq_len, dim)\n",
    "        out = self.W_o(context_vec)\n",
    "        return out\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_dim, emb_dim * 4)\n",
    "        self.fc2 = nn.Linear(emb_dim * 4, emb_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.att = MultiHeadAttention(cfg['emb_dim'], cfg['n_heads'], cfg['context_length'], cfg['drop_rate'], cfg['qkv_bias'])\n",
    "        self.ff = FeedForward(cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        tok_embed = self.tok_emb(input_ids)\n",
    "        pos_embed = self.pos_emb(torch.arange(seq_len, device=input_ids.device))\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        loggits = self.out_head(x)\n",
    "        return loggits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "977a12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model:nn.Module, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        # 取最后一个token\n",
    "        logits = logits[:,-1,:]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48f69006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "# Listing 5.5 Loading OpenAI weights into our GPT model code\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt:GPTModel, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])               #A\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    for b in range(len(params[\"blocks\"])):                                       #B\n",
    "        q_w, k_w, v_w = np.split(                                                #C\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_k.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_v.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_q.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_q.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_k.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_k.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_v.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_v.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.W_o.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.W_o.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_o.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.fc1.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc1.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc1.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.fc2.weight = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.fc2.bias = assign(\n",
    "            gpt.trf_blocks[b].ff.fc2.bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])                   #D\n",
    "\n",
    "\n",
    "#A 将模型的位置嵌入和token 嵌入的权重设置为 params 中指定的值\n",
    "#B 遍历模型中的每个 Transformer 模块\n",
    "#C 使用 np.split 函数将注意力和偏置权重分为三等份，分别用于查询、键和值组件\n",
    "#D OpenAI 的原始 GPT-2 模型在输出层中复用了 token 嵌入的权重，以减少参数总量，这一概念称为权重共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae5b0101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_o): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a65ae216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c42bc4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer), max_new_tokens=35, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "487e1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch) # shape (B,L,V)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.permute(0,2,1), target_batch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c2aaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94806f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.9143, Val loss: 2.6746\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "print(f\"Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3292a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.129, Val loss 1.999\n",
      "Ep 1 (Step 000005): Train loss 1.356, Val loss 1.282\n",
      "Ep 1 (Step 000010): Train loss 0.936, Val loss 1.000\n",
      "Ep 1 (Step 000015): Train loss 0.924, Val loss 0.955\n",
      "Ep 1 (Step 000020): Train loss 0.916, Val loss 0.934\n",
      "Ep 1 (Step 000025): Train loss 0.791, Val loss 0.900\n",
      "Ep 1 (Step 000030): Train loss 0.837, Val loss 0.880\n",
      "Ep 1 (Step 000035): Train loss 0.748, Val loss 0.870\n",
      "Ep 1 (Step 000040): Train loss 0.810, Val loss 0.870\n",
      "Ep 1 (Step 000045): Train loss 0.742, Val loss 0.883\n",
      "Ep 1 (Step 000050): Train loss 0.683, Val loss 0.851\n",
      "Ep 1 (Step 000055): Train loss 0.927, Val loss 0.851\n",
      "Ep 1 (Step 000060): Train loss 0.708, Val loss 0.830\n",
      "Ep 1 (Step 000065): Train loss 0.613, Val loss 0.798\n",
      "Ep 1 (Step 000070): Train loss 0.610, Val loss 0.795\n",
      "Ep 1 (Step 000075): Train loss 0.613, Val loss 0.802\n",
      "Ep 1 (Step 000080): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000085): Train loss 0.543, Val loss 0.787\n",
      "Ep 1 (Step 000090): Train loss 0.552, Val loss 0.802\n",
      "Ep 1 (Step 000095): Train loss 0.502, Val loss 0.807\n",
      "Ep 1 (Step 000100): Train loss 0.539, Val loss 0.787\n",
      "Ep 1 (Step 000105): Train loss 0.629, Val loss 0.769\n",
      "Ep 1 (Step 000110): Train loss 0.606, Val loss 0.764\n",
      "Ep 1 (Step 000115): Train loss 0.505, Val loss 0.761\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the primary function of the human\n",
      "Ep 2 (Step 000120): Train loss 0.455, Val loss 0.767\n",
      "Ep 2 (Step 000125): Train loss 0.439, Val loss 0.777\n",
      "Ep 2 (Step 000130): Train loss 0.482, Val loss 0.772\n",
      "Ep 2 (Step 000135): Train loss 0.353, Val loss 0.783\n",
      "Ep 2 (Step 000140): Train loss 0.392, Val loss 0.792\n",
      "Ep 2 (Step 000145): Train loss 0.381, Val loss 0.784\n",
      "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.771\n",
      "Ep 2 (Step 000155): Train loss 0.439, Val loss 0.782\n",
      "Ep 2 (Step 000160): Train loss 0.435, Val loss 0.772\n",
      "Ep 2 (Step 000165): Train loss 0.380, Val loss 0.769\n",
      "Ep 2 (Step 000170): Train loss 0.341, Val loss 0.771\n",
      "Ep 2 (Step 000175): Train loss 0.395, Val loss 0.770\n",
      "Ep 2 (Step 000180): Train loss 0.444, Val loss 0.747\n",
      "Ep 2 (Step 000185): Train loss 0.479, Val loss 0.745\n",
      "Ep 2 (Step 000190): Train loss 0.390, Val loss 0.758\n",
      "Ep 2 (Step 000195): Train loss 0.276, Val loss 0.744\n",
      "Ep 2 (Step 000200): Train loss 0.252, Val loss 0.733\n",
      "Ep 2 (Step 000205): Train loss 0.289, Val loss 0.725\n",
      "Ep 2 (Step 000210): Train loss 0.321, Val loss 0.721\n",
      "Ep 2 (Step 000215): Train loss 0.391, Val loss 0.721\n",
      "Ep 2 (Step 000220): Train loss 0.279, Val loss 0.728\n",
      "Ep 2 (Step 000225): Train loss 0.256, Val loss 0.736\n",
      "Ep 2 (Step 000230): Train loss 0.228, Val loss 0.735\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef prepares the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Translate 'Thank you' into Mandarin Chinese.\n",
      "Total training time: 1.81 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, eval_freq=5, eval_iter=5, start_context=format_input(val_data[0]), tokenizer=tokenizer)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_min = (end_time - start_time) / 60\n",
    "print(f\"Total training time: {execution_time_min:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06fbab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dab9dfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfc9636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:59<00:00,  1.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(model=model, idx=text_to_token_ids(input_text, tokenizer).to(device), max_new_tokens=256, context_size=BASE_CONFIG['context_length'], eos_id=50256, temperature=0.0)\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "#     json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ee0599b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"Name the author of 'Pride and Prejudice'.\",\n",
       " 'input': '',\n",
       " 'output': 'Jane Austen.',\n",
       " 'model_response': \"The author of 'Pride and Prejudice' is Jane Austen.\"}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "69bb18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"llama3:8b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\":[\n",
    "            {\"role\":\"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\":{\n",
    "            \"seed\":123,\n",
    "            \"temperature\":0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2aba689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(test_data, json_key, model=\"llama3:8b\"):\n",
    "    scores = []\n",
    "    for entry in test_data:\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`,\"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score:{score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53ab0a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores:110 of 110\n",
      "Average score: 54.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores:{len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
